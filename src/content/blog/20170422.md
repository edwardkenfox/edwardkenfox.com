+++
date = "2017-04-22T14:54:39+09:00"
title = "The right performance measurement with webpagetest.org"

+++

<style>
  section.main .content .markdown ol > li {
    list-style-type: decimal;
  }
  section.main .content .markdown li > ul {
    margin: 0;
    padding-left: 1.5em;
  }
</style>

Measuring website performance can be done using the browser’s debug tools, however, they’re not just good enough in some ways. I’ve been using webpagetest.org for this purpose, which has a lot of great features to allow you to efficiently and precisely find the bottleneck of your website performance. Here I would like to explain the basics of webpagetest.org and go measurement results of some websites/applications.

## Benefits of using webpagetest.org

I use Google Chrome for normal browsing as well as debugging. The figures that webpagetest.org provide can mostly be measured with Chrome’s devtools, there are numbers of benefits using webpagetest.org as the below:

1. It allows you to share results with a hyperlink. You'll see some examples later in this post.
1. CPU and Memory throttlings are more accurate.
  - webpagetest.org does remote debugging with actual devices that you selected to measure the performance, so you’ll get more realistic results than using Chrome devtools.
  - According to what I heard from the Google Chrome team, Chrome simulates throttled network only by systematically calling sleep function. However, packet losses, which often happens on slow networks, does not occur with this methodology.
1. It gives you grades of different aspects of the measurement, such as “First Byte Time” or “Cache static content,” so you can quickly find out what the major bottleneck can be.
1. You can download the results and import it to Chrome's devtools.
1. It is generally easier to use.
  - Copying & pasting in Chrome’s Performance tab is such a pain in the ass.

## Basic usage

1. Visit https://www.webpagetest.org.
1. Put the URL of the page you want to measure
1. Select appropriate options from “Test Location” and “Browser"
1. Turn on “First view and repeat view”
  - This is optional, but it is important to see how the page behaves on the second visit to see whether caches are properly used
1. Hit “START TEST” and wait for a bit

For pages that require signing in, you can emulate browser interactions with a special script. For instance, logging in to Twitter would look like:

```
navigate  https://mobile.twitter.com/login
setValue  name=session[username_or_email] YOUR_USER_NAME
setValue  name=session[password] YOUR_PASSWORD
click     type=submit
```

See [the documentation](https://sites.google.com/a/webpagetest.org/docs/using-webpagetest/scripting) for more details of the script.

## Examples

Here are two example results of actual websites I measured using webpagetest.org.

#### 1. Google Maps

- URL: https://www.google.com/maps
- Result: https://www.webpagetest.org/result/170418_ZE_QQH/

You can see the grades at the top right of the page. I guess it’s pretty safe to say Google Maps has got a pretty well-tuned site. Let’s open the [Waterfall page](https://www.webpagetest.org/result/170418_ZE_QQH/1/details/#waterfall_view_step1) and see the details of individual request in order of execution.

You can see that requests 1 to 6 are for the document itself as well as the font files. 7 to 21 are binary image files of the initial map the visitor will see, split into small pieces. Probably due to the connection being HTTP/2, these requests start at the same timing, which is a huge win when you think of how simaltaneous requests were limited to 6 with HTTP/1. 22 to 24 are small icons used for the Google Map menus. A 420KB script file is requested at 25, but looking at the content of the script it seemed like it’s the data of the all the spots on the map containing coordination information. 24 to 58 are also icons on the map. And once again, there are map image files requested after 87, which were the ones of the outer area of the initial view.

With all these facts, you can tell that the requests are organized in a way to display the initial view as quick as possible, even while the whole content of the page has not been downloaded yet. The details of the map or the map images of area that is not visible yet in the first view are loaded lazily after the rest of the requests. This makes the initial rendering of the page fast and reduces the visitor’s stress to wait for the page to be ready for interaction, which is a great practice for better UX.

#### 2. Twitter (mobile)

- URL: https://mobile.twitter.com/
- Result: https://www.webpagetest.org/result/170418_RE_e684110ba5b0608a82c610233c817cd8/

Next, we’re gonna look at Twitter. Twitter announced [Twitter Lite](https://blog.twitter.com/2017/introducing-twitter-lite) recently, but let’s see how fast Twitter’s mobile page is.
I’ve chosen “Moto G (gen 4)” from the device options and “Moto G4 – Chrome” from the browsers. The login page is not the most exciting thing to study, so I prepared a script that allows the test to go see my timeline in the home screen with a new account I created for this purpose.

You’ll see As in the grades sections, except for “Cache Static Content” having grade C. webpagetest.org must have taken average of the results of the login page and the timeline page, and I assume that there aren’t many cacheable contents in the timeline which is why it’s graded C. Just looking at the overall grades, it’s pretty much the same as Google Maps, yet when looking more closely at the actual time Twitter is way faster. There are fewer resources requested compared to Google Maps, so it might not be fair to compare the two, but it's pretty amazing how fast Twitter modile is. This must be a result of continuous, delicate performance tuning Twitter has been doing, especially seeing how fast “DOM interactive” or “Document Complete” are.

Take a look at the [Waterfall view](https://www.webpagetest.org/result/170418_RE_e684110ba5b0608a82c610233c817cd8/1/details/#step2_request4). It shows that small pieces of data of the unread notifications, direct messages and UI components are retrieved in separate API calls. The [Filmstrip view](https://www.webpagetest.org/video/compare.php?tests=170418_RE_e684110ba5b0608a82c610233c817cd8-r:1-c:0), which shows the actual rendering of the screen taken every 0.5 second, is also quite interesting; the splash screen with the bluebird logo is displayed at 1.0 while the rest of the page content is being downloaded. Similar to Google Maps, it’s intended to present the initial view to the user as early as possible. The splash screen is pretty meaningless, but it's certainly better than a total blank page, right?

## Things to keep in mind when tuning website performance

The right approach to make your website/application faster varies heavily depending on its content, but the below are the things to keep in mind in general.

- Organize the order of the requests so that the initial view is shown to the user as early as possible
  - Compare the Filmstrip view and the request waterfall. Adjust the order by making the requests for resources necessary for the first view to start towards the beginning.
  - Resources that will be needed eventually upon user interaction, or third-party widgets, can be pushed to later of the page lifecycle.
    - `defer` attribute can be useful for this
- Start the download of files that interfere the critical rendering paths of the browser, such as JavaScript or CSS, as early as possible to make the document interactive sooner.
  - `preload` or `prefetch` attributes may be used for this, but abusing them may actually harm the performance.
- Effectively use CDNs.
- Use HTTP/2 to allow the browser to perform pipelined stream requests.
- Reduce the number of resources and make each of them smaller.
  - Concatenate, minify, compress JavaScript & CSS files
  - Compress image files
- Check if there’s any unused resources requested or loaded.
- Use client-side cache
  - Use proper response headers
  - Use ServiceWorkers
